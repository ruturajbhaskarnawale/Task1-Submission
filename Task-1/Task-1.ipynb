{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e88df6-416c-4b6b-b86d-40ec37098fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and outputs directory created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries and create output directory\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Create output directory to store results\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported and outputs directory created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d18cbd0e-f894-4db9-b095-8275141d2835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokenizer and model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"BERT tokenizer and model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcab079f-91c8-4743-aefd-4f594fd5cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text (Input IDs): tensor([[  101,  2023,  2003,  1037,  7099,  3793,  2005, 19204,  3989,  1998,\n",
      "         17181,  1012,   102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Token Type IDs: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define input text and tokenize\n",
    "text = \"This is a sample text for tokenization and encoding.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    padding=True,         # Add padding if needed\n",
    "    truncation=True,      # Truncate if text exceeds max length\n",
    "    max_length=128        # Set max token length\n",
    ")\n",
    "\n",
    "print(\"Tokenized Text (Input IDs):\", inputs['input_ids'])\n",
    "print(\"Attention Mask:\", inputs['attention_mask'])\n",
    "print(\"Token Type IDs:\", inputs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81777c1-1aa2-4fe1-bb3e-83c7c29d4e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Shape: torch.Size([1, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Encode the text to get embeddings\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  # Shape: [batch_size, seq_length, hidden_size]\n",
    "\n",
    "print(\"Embeddings Shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c073bdd8-a4e3-441a-9fc7-490cb30e1dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized inputs and embeddings saved to outputs folder.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Save tokenized inputs and embeddings\n",
    "torch.save(inputs, 'outputs/task1_inputs.pt')\n",
    "torch.save(embeddings, 'outputs/task1_embeddings.pt')\n",
    "\n",
    "print(\"Tokenized inputs and embeddings saved to outputs folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54a77529-4cc7-420a-ba56-34aa49424197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Tokens: ['[CLS]', 'this', 'is', 'a', 'sample', 'text', 'for', 'token', '##ization', 'and', 'encoding', '.', '[SEP]']\n",
      "Summary saved to task1_summary.txt.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Decode tokens and save summary\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(\"Decoded Tokens:\", decoded_tokens)\n",
    "\n",
    "with open('outputs/task1_summary.txt', 'w') as f:\n",
    "    f.write(f\"Input Text: {text}\\n\")\n",
    "    f.write(f\"Tokenized Input IDs: {inputs['input_ids'].tolist()}\\n\")\n",
    "    f.write(f\"Decoded Tokens: {decoded_tokens}\\n\")\n",
    "    f.write(f\"Embeddings Shape: {embeddings.shape}\\n\")\n",
    "\n",
    "print(\"Summary saved to task1_summary.txt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "344c1ef8-5365-4c88-bbad-29826ce66722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Inputs: {'input_ids': tensor([[  101,  2023,  2003,  1037,  7099,  3793,  2005, 19204,  3989,  1998,\n",
      "         17181,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Loaded Embeddings Shape: torch.Size([1, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 7: Verify loaded files\n",
    "import torch\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "import torch.serialization\n",
    "\n",
    "# Add the BatchEncoding class to the safe globals list\n",
    "torch.serialization.add_safe_globals([BatchEncoding])\n",
    "\n",
    "# Now load the files\n",
    "inputs = torch.load('outputs/task1_inputs.pt')\n",
    "embeddings = torch.load('outputs/task1_embeddings.pt')\n",
    "\n",
    "print(\"Loaded Inputs:\", inputs)\n",
    "print(\"Loaded Embeddings Shape:\", embeddings.shape)\n",
    "\n",
    "# Alternative approach (if the above doesn't work):\n",
    "# inputs = torch.load('outputs/task1_inputs.pt', weights_only=False)\n",
    "# embeddings = torch.load('outputs/task1_embeddings.pt', weights_only=False)\n",
    "# Note: Only use weights_only=False if you trust the source of these files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a421dea-2247-42d9-9e09-20665b37686e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
